{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(cmd):\n",
    "    return subprocess.check_output(cmd, shell=True).decode(sys.stdout.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version to support fst-output\n",
    "def read_fst4conll(fst_file, fs=\"\\t\", oov='<unk>', otag='O', sep='+', split=False):\n",
    "    \"\"\"\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :param oov: token to map to otag (we need to get rid of <unk> in labels)\n",
    "    :param otag: otag symbol\n",
    "    :param sep: \n",
    "    :param split:\n",
    "    :return: corpus \n",
    "    \"\"\"\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(fst_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            # arc has minimum 3 columns, else final state\n",
    "            if len(feats) >= 3:\n",
    "                ist = feats[2]  # 3rd column (input)\n",
    "                ost = feats[3]  # 4th column (output)\n",
    "                # replace '<unk>' with 'O'\n",
    "                ost = otag if ost == oov else ost\n",
    "                # ignore for now\n",
    "                ost = ost.split(sep)[1] if split and ost != otag else ost\n",
    "                \n",
    "                words.append((ist, ost))\n",
    "            else:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words) \n",
    "                words = []\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
    "    \"\"\"\n",
    "    read corpus in CoNLL format\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :return: corpus\n",
    "    \"\"\"\n",
    "    featn = None  # number of features for consistency check\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(corpus_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            if not featn:\n",
    "                featn = len(feats)\n",
    "            elif featn != len(feats) and len(feats) != 0:\n",
    "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
    "\n",
    "            words.append(feats)\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_list(corpus):\n",
    "    \"\"\"\n",
    "    create frequency list for a corpus\n",
    "    :param corpus: corpus as list of lists\n",
    "    \"\"\"\n",
    "    frequencies = {}\n",
    "    for sent in corpus:\n",
    "        for token in sent:\n",
    "            frequencies[token] = frequencies.setdefault(token, 0) + 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(corpus, tf_min=2):\n",
    "    \"\"\"\n",
    "    apply min cutoffs\n",
    "    :param tf_min: minimum token frequency for lexicon elements (below removed); default 2\n",
    "    :return: lexicon as set\n",
    "    \"\"\"\n",
    "    frequencies = compute_frequency_list(corpus)\n",
    "    return sorted([token for token, frequency in frequencies.items() if frequency >= tf_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'tmp/'\n",
    "wdir = temp_folder + 'wdir_wt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = 'dataset/'\n",
    "\n",
    "def init():\n",
    "    execute('mkdir -p ' + temp_folder)\n",
    "\n",
    "    execute('cp {}NL2SparQL4NLU.train.utterances.txt {}trn.txt'.format(dpath, temp_folder))\n",
    "    execute('cp {}NL2SparQL4NLU.test.utterances.txt {}tst.txt'.format(dpath, temp_folder))\n",
    "\n",
    "    execute('cp {}NL2SparQL4NLU.train.conll.txt {}trn.conll'.format(dpath, temp_folder))\n",
    "    execute('cp {}NL2SparQL4NLU.test.conll.txt {}tst.conll'.format(dpath, temp_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(min_freq=2):\n",
    "    # create training data in utterance-per-line format for output symbols (w+t)\n",
    "    trn = read_corpus_conll(temp_folder + 'trn.conll')\n",
    "    wt_sents = [[\"+\".join(w) for w in s] for s in trn]\n",
    "    wt_osyms = cutoff(wt_sents, min_freq)\n",
    "    wt_isyms = [w.split('+')[0] for w in wt_osyms]\n",
    "\n",
    "    with open(temp_folder + 'trn.wt.txt', 'w') as f:\n",
    "        for s in wt_sents:\n",
    "            f.write(\" \".join(s) + \"\\n\")\n",
    "\n",
    "    with open(temp_folder + 'osyms.wt.lst.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(wt_osyms) + \"\\n\")\n",
    "\n",
    "    with open(temp_folder + 'isyms.wt.lst.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(wt_isyms) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symbol_table():\n",
    "    execute('ngramsymbols {0}osyms.wt.lst.txt {0}osyms.wt.txt'.format(temp_folder))\n",
    "    execute('ngramsymbols {0}isyms.wt.lst.txt {0}isyms.wt.txt'.format(temp_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FAR():\n",
    "    execute(\"farcompilestrings \\\n",
    "        --symbols={0}osyms.wt.txt \\\n",
    "        --keep_symbols \\\n",
    "        --unknown_symbol='<unk>' \\\n",
    "        {0}trn.wt.txt {0}trn.wt.far\".format(temp_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(ngram_order):\n",
    "    execute('ngramcount --order={1} {0}trn.wt.far {0}trn.wt.cnt'.format(temp_folder, ngram_order))\n",
    "    execute('ngrammake {0}trn.wt.cnt {0}wt2.lm'.format(temp_folder))\n",
    "    execute('ngraminfo {0}wt2.lm'.format(temp_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t_wt(isyms, sep='+', out=temp_folder+'w2wt.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'  # unknown symbol\n",
    "    state = '0'    # wfst specification state\n",
    "    fs = \" \"       # wfst specification column separator\n",
    "    \n",
    "    ist = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special))\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for e in ist:\n",
    "            f.write(fs.join([state, state, e.split(sep)[0], e]) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_w2wt_wt():\n",
    "    execute('fstcompile \\\n",
    "        --isymbols={0}isyms.wt.txt \\\n",
    "        --osymbols={0}osyms.wt.txt \\\n",
    "        --keep_isymbols \\\n",
    "        --keep_osymbols \\\n",
    "        {0}w2wt_wt.txt {0}w2wt_wt.bin'.format(temp_folder))\n",
    "\n",
    "    #info = execute('fstinfo {0}w2wt_wt.bin | head -n 8'.format(temp_folder))\n",
    "    #print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_strings_and_extract():\n",
    "    execute(\"farcompilestrings \\\n",
    "        --symbols={0}isyms.wt.txt \\\n",
    "        --keep_symbols \\\n",
    "        --initial_symbols=false \\\n",
    "        --unknown_symbol='<unk>' \\\n",
    "        {0}tst.txt {0}tst.wt.far\".format(temp_folder))\n",
    "\n",
    "    execute('rm -r ' + wdir)\n",
    "    execute('mkdir ' + wdir)\n",
    "\n",
    "    execute('farextract --filename_prefix=\"{1}\" {0}tst.wt.far'.format(temp_folder, wdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_fst():\n",
    "    fst_files = [f for f in os.listdir(wdir) if os.path.isfile(os.path.join(wdir, f))]\n",
    "\n",
    "    fst_out = ''\n",
    "\n",
    "    for f in sorted(fst_files):\n",
    "        tmp = execute('fstcompose {1}{2} {0}w2wt_wt.bin | fstcompose - {0}wt2.lm |\\\n",
    "            fstshortestpath | fstrmepsilon | fsttopsort | fstprint --isymbols={0}isyms.wt.txt'\n",
    "                  .format(temp_folder, wdir, f))\n",
    "        if(len(tmp) == 0):\n",
    "            print(\"empty\", f)\n",
    "            \n",
    "        fst_out += tmp\n",
    "\n",
    "    with open(temp_folder + 'w2wt_wt.wt2.out', 'w+') as f:\n",
    "        f.write(fst_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(refs, hyps):\n",
    "    assert len(refs) == len(hyps)\n",
    "\n",
    "    correct_tags = 0\n",
    "    wrong_tags = 0\n",
    "    \n",
    "    for i in range(len(refs)):\n",
    "        ref = refs[i]\n",
    "        hyp = hyps[i]\n",
    "        assert len(ref) == len(hyp)        \n",
    "        \n",
    "        for j in range(len(ref)):\n",
    "            if hyp[j][1] == ref[j][1]:\n",
    "                correct_tags += 1\n",
    "            else:\n",
    "                wrong_tags += 1\n",
    "\n",
    "\n",
    "    return correct_tags / (correct_tags + wrong_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(refs, hyps, show_table=False):\n",
    "    acc = compute_accuracy(refs, hyps)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    results = evaluate(refs, hyps)\n",
    "\n",
    "    print(\"Precision:\", results['total']['p'])\n",
    "    print(\"Recall:\", results['total']['r'])    \n",
    "    print(\"F1-score:\", results['total']['f'])\n",
    "    \n",
    "    if show_table:\n",
    "        pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "        return pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fst():\n",
    "    refs = read_corpus_conll(temp_folder + 'tst.conll')\n",
    "    hyps = read_fst4conll(temp_folder + 'w2wt_wt.wt2.out', split=True)\n",
    "    \n",
    "    \n",
    "    return show_metrics(refs, hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SCLM(min_freq, ngram_degree):\n",
    "    create_training_data(min_freq=min_freq)\n",
    "    create_symbol_table()\n",
    "    compile_FAR()\n",
    "    train_ngram_model(ngram_degree)\n",
    "    \n",
    "    make_w2t_wt(temp_folder + 'osyms.wt.txt', out=temp_folder + 'w2wt_wt.txt')\n",
    "    \n",
    "    compile_w2wt_wt()\n",
    "    compile_strings_and_extract()\n",
    "    \n",
    "    compose_fst()\n",
    "    \n",
    "    return evaluate_fst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllChunkCorpusReader\n",
    "import nltk.tag.hmm as hmm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iob(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, None)\n",
    "\n",
    "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
    "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
    "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_hmm():\n",
    "    trn = dpath + 'NL2SparQL4NLU.train.conll.txt'\n",
    "    concepts = sorted(get_chunks(trn))\n",
    "\n",
    "    trn_data = ConllChunkCorpusReader(dpath,  r'NL2SparQL4NLU.train.conll.txt', concepts)\n",
    "    tst_data = ConllChunkCorpusReader(dpath,  r'NL2SparQL4NLU.test.conll.txt', concepts)\n",
    "    \n",
    "    return trn_data, tst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(trn_data):\n",
    "    hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "    hmm_tagger = hmm_model.train(trn_data.tagged_sents())\n",
    "    return hmm_tagger\n",
    "\n",
    "    \n",
    "def evaluate_hmm(hmm_tagger, tst_data):\n",
    "    accuracy = hmm_tagger.evaluate(tst_data.tagged_sents())\n",
    "    \n",
    "    refs = [s for s in tst_data.tagged_sents()]\n",
    "    hyps = [hmm_tagger.tag(s) for s in tst_data.sents()]\n",
    "\n",
    "    return show_metrics(refs, hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_HMM():\n",
    "    trn_data, tst_data = load_dataset_hmm()\n",
    "    hmm_tagger = train_hmm(trn_data)\n",
    "    return evaluate_hmm(hmm_tagger, tst_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9158353238724182\n",
      "Precision: 0.7532210109018831\n",
      "Recall: 0.696608615948671\n",
      "F1-score: 0.7238095238095239\n"
     ]
    }
   ],
   "source": [
    "compute_SCLM(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9086693831670648\n",
      "Precision: 0.7719112988384371\n",
      "Recall: 0.6700274977085243\n",
      "F1-score: 0.717369970559372\n"
     ]
    }
   ],
   "source": [
    "compute_HMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
